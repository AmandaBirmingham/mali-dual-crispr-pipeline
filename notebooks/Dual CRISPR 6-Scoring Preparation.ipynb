{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual CRISPR Screen Analysis\n",
    "# Scoring Preparation\n",
    "Amanda Birmingham, CCBB, UCSD (abirmingham@ucsd.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instructions\n",
    "\n",
    "To run this notebook reproducibly, follow these steps:\n",
    "1. Click **Kernel** > **Restart & Clear Output**\n",
    "2. When prompted, click the red **Restart & clear all outputs** button\n",
    "3. Fill in the values for your analysis for each of the variables in the [Input Parameters](#input-parameters) section\n",
    "4. Click **Cell** > **Run All**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_timestamp = \"\"\n",
    "g_dataset_name = \"test\"\n",
    "\n",
    "# TODO: put back to plain assignments before using with nbparameterise\n",
    "g_code_location = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/ideker-dual-crispr-software'\n",
    "    '/src/python')\n",
    "g_base_dir = \"{0}{1}\".format(g_code_location, \"/test_files\")\n",
    "g_constructs_fp = \"{0}{1}\".format(g_base_dir, '/known_inputs/Metabolism_dual_spacers_w_probe_names.txt')\n",
    "g_col_indices_str = \"1,3,6,7,8,11,12\"\n",
    "g_count_fps_or_dirs = \"{0}{1}\".format(g_base_dir, '/known_inputs/20160706_HeLa_A549_MV4_19mer_1mm_py_20160713235254')\n",
    "g_scoring_ready_counts_dir = \"{0}{1}\".format(g_base_dir, '/test_outputs')\n",
    "\n",
    "\n",
    "# TODO: decide which (if any) of these I need\n",
    "g_scoring_run_prefix = \"\"\n",
    "g_project_name = \"test_proj\"\n",
    "g_day_timepoints_str = \"3,14,20,28\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCBB Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(g_code_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s describe_var_list /Users/Birmingham/Repositories/ccbb_tickets/20160210_mali_crispr/src/python/ccbbucsd/utilities/analysis_run_prefixes.py\n",
    "def describe_var_list(input_var_name_list):\n",
    "    description_list =  [\"{0}: {1}\\n\".format(name, eval(name)) for name in input_var_name_list]\n",
    "    return \"\".join(description_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s get_combined_counts_file_suffix /Users/Birmingham/Repositories/ccbb_tickets/20160210_mali_crispr/src/python/ccbbucsd/malicrispr/count_combination.py\n",
    "def get_combined_counts_file_suffix():\n",
    "    return \"counts_combined.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_timestamp: 20170307094751\n",
      "g_count_file_fps: ['/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/ideker-dual-crispr-software/src/python/test_files/known_inputs/20160706_HeLa_A549_MV4_19mer_1mm_py_20160713235254/20160706_HeLa_A549_MV4_19mer_1mm_py_20160713235254_mod_counts_combined.txt']\n",
      "g_col_indices: [1, 3, 6, 7, 8, 11, 12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from ccbbucsd.utilities.analysis_run_prefixes import check_or_set, get_run_prefix, get_timestamp, get_ints_from_comma_sep_string\n",
    "from ccbbucsd.utilities.files_and_paths import get_filepaths_from_wildcard\n",
    "\n",
    "def get_count_file_fps(comma_sep_fps_or_dirs_str):\n",
    "    result = []\n",
    "    \n",
    "    fps_or_dirs = comma_sep_fps_or_dirs_str.split(\",\")\n",
    "    for curr_fp_or_dir in fps_or_dirs:\n",
    "        trimmed_curr = curr_fp_or_dir.strip()\n",
    "        if os.path.isdir(trimmed_curr):\n",
    "            combined_counts_fps = get_filepaths_from_wildcard(trimmed_curr, \n",
    "                get_combined_counts_file_suffix())    \n",
    "            result.extend(combined_counts_fps)\n",
    "        else:\n",
    "            result.append(trimmed_curr)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "\n",
    "g_timestamp = check_or_set(g_timestamp, get_timestamp())\n",
    "g_count_file_fps = get_count_file_fps(g_count_fps_or_dirs)\n",
    "g_col_indices = get_ints_from_comma_sep_string(g_col_indices_str)\n",
    "print(describe_var_list(['g_timestamp','g_count_file_fps', 'g_col_indices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ccbbucsd.utilities.files_and_paths import verify_or_make_dir\n",
    "verify_or_make_dir(g_scoring_ready_counts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count File Merging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /Users/Birmingham/Work/Repositories/ccbb_tickets/20161100_mali_crispr_software/src/python/ccbbucsd/malicrispr/construct_file_extracter.py\n",
    "# third-party libraries\n",
    "import pandas\n",
    "\n",
    "# ccbb libraries\n",
    "from ccbbucsd.utilities.bio_seq_utilities import trim_seq\n",
    "\n",
    "__author__ = \"Amanda Birmingham\"\n",
    "__maintainer__ = \"Amanda Birmingham\"\n",
    "__email__ = \"abirmingham@ucsd.edu\"\n",
    "__status__ = \"prototype\"\n",
    "\n",
    "_CONSTRUCT_ID = \"construct_id\"\n",
    "_PROBE_A_SEQ = \"probe_a_seq\"\n",
    "_PROBE_B_SEQ = \"probe_b_seq\"\n",
    "_PROBE_A_NAME = \"probe_a_id\"\n",
    "_PROBE_B_NAME = \"probe_b_id\"\n",
    "_TARGET_A_NAME = \"target_a_id\"\n",
    "_TARGET_B_NAME = \"target_b_id\"\n",
    "\n",
    "\n",
    "def get_construct_header():\n",
    "    return _CONSTRUCT_ID\n",
    "\n",
    "\n",
    "def get_probe_id_header(probe_letter):\n",
    "    return _PROBE_A_NAME if _is_letter_a(probe_letter) else _PROBE_B_NAME\n",
    "\n",
    "\n",
    "def get_probe_seq_header(probe_letter):\n",
    "    return _PROBE_A_SEQ if _is_letter_a(probe_letter) else _PROBE_B_SEQ\n",
    "\n",
    "\n",
    "def get_target_id_header(target_letter):\n",
    "    return _TARGET_A_NAME if _is_letter_a(target_letter) else _TARGET_B_NAME\n",
    "\n",
    "\n",
    "def extract_construct_and_grna_info(constructs_fp, column_indices):\n",
    "    construct_table = _read_in_construct_table(constructs_fp, column_indices, rows_to_skip=1)\n",
    "    seq_name_sets = _extract_unique_sets_across_a_and_b(construct_table,\n",
    "        [_PROBE_A_NAME, _PROBE_A_SEQ], [_PROBE_B_NAME, _PROBE_B_SEQ])\n",
    "    probe_name_seq_pairs = _validate_and_format_probe_seq_pairs(seq_name_sets)\n",
    "    construct_names = construct_table[_CONSTRUCT_ID].unique().tolist()\n",
    "    return construct_names, probe_name_seq_pairs\n",
    "\n",
    "\n",
    "def trim_probes(probes_name_and_seq_list, retain_len):\n",
    "    result = []\n",
    "    for name_seq_tuple in probes_name_and_seq_list:\n",
    "        probe_name = name_seq_tuple[0]\n",
    "        full_seq = name_seq_tuple[1]\n",
    "        trimmed_seq = trim_seq(full_seq, retain_len, False)  # False = do not retain from 5p end but from 3p end\n",
    "        result.append((probe_name, trimmed_seq))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _is_letter_a(letter):\n",
    "    if letter.upper() == \"A\":\n",
    "        result = True\n",
    "    elif letter.upper() == \"B\":\n",
    "        result = False\n",
    "    else:\n",
    "        raise ValueError(\"Input '{0}' is not recognized as A or B.\".format(letter))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _read_in_construct_table(constructs_fp, column_indices, rows_to_skip=1):\n",
    "    result = pandas.read_table(constructs_fp, skiprows=rows_to_skip, header=None)\n",
    "    result = _rename_columns(result, column_indices)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _rename_columns(construct_table, column_indices):\n",
    "    new_names = [_CONSTRUCT_ID, _TARGET_A_NAME, _PROBE_A_NAME, _PROBE_A_SEQ,\n",
    "                 _TARGET_B_NAME, _PROBE_B_NAME, _PROBE_B_SEQ]\n",
    "    existing_names = list(construct_table.columns.values)\n",
    "\n",
    "    if len(column_indices) != len(new_names):\n",
    "        raise ValueError(\"Expected indices for {0} columns but received indices for {1}.\".format(\n",
    "            len(new_names), len(column_indices)))\n",
    "\n",
    "    existing_to_new_names = {}\n",
    "    for curr_index in range(0, len(column_indices)):\n",
    "        curr_col_index = column_indices[curr_index]\n",
    "        curr_existing_name = existing_names[curr_col_index]\n",
    "        existing_to_new_names[curr_existing_name] = new_names[curr_index]\n",
    "\n",
    "    return construct_table.rename(columns=existing_to_new_names)\n",
    "\n",
    "\n",
    "def _extract_unique_sets_across_a_and_b(construct_table, a_col_headers_list, b_col_headers_list):\n",
    "    if len(a_col_headers_list) != len(b_col_headers_list):\n",
    "        raise ValueError(\"A and B column header lists are not equal in length.\")\n",
    "\n",
    "    new_headers_list = [\"temp_header_{0}\".format(i) for i in range(0,len(a_col_headers_list))]\n",
    "\n",
    "    # get the set of input columns for each of the two targets, assigning each the same\n",
    "    # set of (generic) column headers so that they can easily be concatenated\n",
    "    set_for_a = _extract_renamed_subset_df(construct_table, a_col_headers_list, new_headers_list)\n",
    "    set_for_b = _extract_renamed_subset_df(construct_table, b_col_headers_list, new_headers_list)\n",
    "    combined_set = pandas.concat([set_for_a, set_for_b])\n",
    "\n",
    "    # extract only the unique sets\n",
    "    grouped_combined_set = combined_set.groupby(new_headers_list).groups\n",
    "    result = [x for x in grouped_combined_set]\n",
    "    return sorted(result)  # NB sort so that output order is predictable\n",
    "\n",
    "\n",
    "def _extract_renamed_subset_df(construct_table, col_headers_list, new_headers_list):\n",
    "    result = construct_table[col_headers_list]\n",
    "    rename_dictionary = dict(zip(col_headers_list, new_headers_list))\n",
    "    result.rename(columns=rename_dictionary, inplace=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _validate_and_format_probe_seq_pairs(probes_seq_and_name_list):\n",
    "    expected_num_pieces = 2\n",
    "    seqs_by_names = {}\n",
    "    names_by_seqs = {}\n",
    "    result = []\n",
    "\n",
    "    for curr_set in probes_seq_and_name_list:\n",
    "        if len(curr_set) != expected_num_pieces:\n",
    "            raise ValueError(\n",
    "                \"input '{0}' has {1} pieces instead of the expected {2}\".format(\n",
    "                    curr_set, len(curr_set), expected_num_pieces\n",
    "                ))\n",
    "        curr_seq = curr_set[0]\n",
    "        curr_name = curr_set[1]\n",
    "\n",
    "        if curr_seq in names_by_seqs:\n",
    "            raise ValueError(\n",
    "                \"sequence '{0}' associated with name '{1}' but was already associated with name '{2}'\".format(\n",
    "                    curr_seq, curr_name, names_by_seqs[curr_seq]\n",
    "                ))\n",
    "\n",
    "        if curr_name in seqs_by_names:\n",
    "            raise ValueError(\n",
    "                \"name '{0}' associated with sequence '{1}' but was already associated with sequence '{2}'\".format(\n",
    "                    curr_name, curr_seq, seqs_by_names[curr_name]\n",
    "                ))\n",
    "\n",
    "        names_by_seqs[curr_seq] = curr_name\n",
    "        seqs_by_names[curr_name] = curr_seq\n",
    "\n",
    "        result.append((curr_name, curr_seq.upper())) # upper-case all probe seqs\n",
    "    # next pair in\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _alphabetize_two_fields_in_row(input_df, header_of_col_to_be_first, header_of_col_to_be_second):\n",
    "    is_row_dealphabetized = input_df[header_of_col_to_be_first] >input_df[header_of_col_to_be_second] # boolean array\n",
    "\n",
    "    # for dealphabetized rows, get header_of_col_to_be_first's value\n",
    "    orig_first_col_vals_for_out_of_order_rows = input_df.loc[is_row_dealphabetized, header_of_col_to_be_first]\n",
    "\n",
    "    # replace header_of_col_to_be_first's value with header_of_col_to_be_second's value for dealphabetized rows\n",
    "    input_df.loc[is_row_dealphabetized, header_of_col_to_be_first] = input_df.loc[is_row_dealphabetized,\n",
    "                                                                                header_of_col_to_be_second]\n",
    "    # replace header_of_col_to_be_second's value with header_of_col_to_be_first's original value for dealphabetized rows\n",
    "    input_df.loc[is_row_dealphabetized, header_of_col_to_be_second] = orig_first_col_vals_for_out_of_order_rows\n",
    "    return input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load /Users/Birmingham/Work/Repositories/ccbb_tickets/20161100_mali_crispr_software/src/python/ccbbucsd/malicrispr/scoring_prep.py\n",
    "# standard libraries\n",
    "import re\n",
    "\n",
    "# ccbb libraries\n",
    "from ccbbucsd.utilities.pandas_utils import add_series_to_dataframe\n",
    "\n",
    "# project-specific libraries\n",
    "from ccbbucsd.malicrispr.construct_file_extracter import get_target_id_header, \\\n",
    "    get_probe_id_header, get_construct_header, get_target_pair_id_header, \\\n",
    "    get_probe_pair_id_header\n",
    "\n",
    "__author__ = \"Amanda Birmingham\"\n",
    "__maintainer__ = \"Amanda Birmingham\"\n",
    "__email__ = \"abirmingham@ucsd.edu\"\n",
    "__status__ = \"development\"\n",
    "\n",
    "\n",
    "_HEADER_DIVIDER = \"_\"\n",
    "_TIME_PREFIX = \"T\"\n",
    "_NUM_HEADER_PIECES = 3\n",
    "\n",
    "\n",
    "def get_header_divider():\n",
    "    return _HEADER_DIVIDER\n",
    "\n",
    "\n",
    "def get_time_prefix():\n",
    "    return _TIME_PREFIX\n",
    "\n",
    "\n",
    "def _get_num_header_pieces():\n",
    "    return _NUM_HEADER_PIECES\n",
    "\n",
    "\n",
    "def _clip_count_header_suffix(count_header):\n",
    "    # if count header comes out of Amanda's count pipeline, it will have\n",
    "    # \"_S#+_trimmed53_len_filtered_counts\" on the end of it; get rid of this.\n",
    "    # if it didn't come out of Amanda's pipeline, it won't have this particular\n",
    "    # suffix and the below trimming will simply have no effect.\n",
    "    #\n",
    "    # Regex key:\n",
    "    # r means following is a raw string--many special characters ignored\n",
    "    # [0-9]+ means \"at least one digit, maybe more\"\n",
    "    # $ means \"end of string\"\n",
    "    pipeline_counts_regex = r'_S[0-9]+_trimmed53_len_filtered_counts$'\n",
    "    result = re.sub(pipeline_counts_regex, \"\", count_header)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _validate_and_decompose_count_header(count_header):\n",
    "    # Required count header format: experiment_timept_rep\n",
    "    num_expected_pieces = _get_num_header_pieces()\n",
    "    divider = get_header_divider()\n",
    "    trimmed_count_header = _clip_count_header_suffix(count_header)\n",
    "\n",
    "    count_header_pieces = trimmed_count_header.split(divider)\n",
    "    return _validate_and_standardize_count_header_pieces(count_header_pieces)\n",
    "\n",
    "\n",
    "def _validate_and_standardize_count_header_pieces(count_header_pieces):\n",
    "    # Required count header format: experiment_timept_rep\n",
    "    num_expected_pieces = _get_num_header_pieces()\n",
    "    if len(count_header_pieces) != num_expected_pieces:\n",
    "        raise ValueError(\"Count header has {0} pieces instead of the expected {1}: '{2}'.\",\n",
    "            len(count_header_pieces), num_expected_pieces, count_header_pieces)\n",
    "\n",
    "    some_id = count_header_pieces[0]\n",
    "    timept = _validate_and_standardize_timepoint(count_header_pieces[1])\n",
    "    replicate = _validate_and_standardize_replicate(count_header_pieces[2])\n",
    "\n",
    "    return (some_id, timept, replicate)\n",
    "\n",
    "\n",
    "def _validate_and_standardize_timepoint(timept):\n",
    "    if isinstance(timept, str):\n",
    "        # ensure timepoint is \"t\" or \"T\" plus a non-negative integer number\n",
    "        expected_timepoint_prefix = get_time_prefix()\n",
    "        timepoint_prefix = timept[:1]\n",
    "        if timepoint_prefix.upper() != expected_timepoint_prefix.upper():\n",
    "            raise ValueError(\"Time point '{0}' does not start with '{1}' or '{2}'.\", timept,\n",
    "                expected_timepoint_prefix.lower(), expected_timepoint_prefix.upper())\n",
    "\n",
    "        timept = timept[1:]\n",
    "    else:\n",
    "        timept = str(timept)\n",
    "\n",
    "    if not timept.isdigit():\n",
    "        raise ValueError(\"Time point value '{0}' is not recognizable as a positive integer.\",\n",
    "                         timept)\n",
    "\n",
    "    return int(timept)\n",
    "\n",
    "\n",
    "def _validate_and_standardize_replicate(rep):\n",
    "    if not isinstance(rep, int):\n",
    "        rep = int(rep) if rep.isdigit() else rep\n",
    "    return rep\n",
    "\n",
    "\n",
    "def _validate_expt_structure(expt_structure_by_id):\n",
    "    # expt_structure_by_id should have format {some_id: {timept: {set of replicates}}}\n",
    "\n",
    "    # There must be only one experiment represented in the data structure\n",
    "\n",
    "    # All timepoints in the experiment must have the exact same set of replicates:\n",
    "    # e.g., can't have sample1_T1_1; sample1_T2_1, sample1_T2_2\n",
    "\n",
    "    if len(expt_structure_by_id) != 1:\n",
    "        raise ValueError((\"Count headers must describe one and only one experiment, \"\n",
    "            \"but {0} were detected: '{1}'.\").format(len(expt_structure_by_id),\n",
    "            sorted(list(expt_structure_by_id.keys()))))\n",
    "\n",
    "    for curr_expt_id, curr_expt_structure in expt_structure_by_id.items():\n",
    "        # ensure all timepoints for current sample have the same number of replicates\n",
    "        is_first_timept = True\n",
    "        reference_reps_set = None\n",
    "\n",
    "        if len(curr_expt_structure) == 0:\n",
    "            raise ValueError(\"Count headers must describe at least one timepoint for experiment, \"\n",
    "                \"but 0 were detected.\")\n",
    "\n",
    "        for curr_timept, curr_rep_set in curr_expt_structure.items():\n",
    "            if len(curr_rep_set) == 0:\n",
    "                raise ValueError((\"Count headers must describe at least one replicate for each timepoint, \"\n",
    "                                  \"but 0 were detected for timepoint '{0}'.\").format(curr_timept))\n",
    "\n",
    "            if is_first_timept:\n",
    "                reference_reps_set = curr_rep_set\n",
    "                is_first_timept = False\n",
    "            else:\n",
    "                if curr_rep_set != reference_reps_set:\n",
    "                    raise ValueError(\"For sample '{0}', timepoint {1} has \"\n",
    "                        \"replicates '{2}' instead of the expected '{3}'\".format(\n",
    "                        curr_expt_id, curr_timept, sorted(curr_rep_set),\n",
    "                        sorted(reference_reps_set)))\n",
    "\n",
    "\n",
    "def validate_and_parse_data_column_headers(count_headers):\n",
    "    # ensure that there is only one experiment represented in headers\n",
    "    # ensure that every timepoint in the experiment has the exact same set of replicates\n",
    "\n",
    "    expt_structure_by_id = {}\n",
    "    result = []\n",
    "\n",
    "    for curr_count_header in count_headers:\n",
    "        # Required count header format: experiment_timept_rep\n",
    "        count_header_pieces = _validate_and_decompose_count_header(curr_count_header)\n",
    "        some_id = count_header_pieces[0]\n",
    "        timept = count_header_pieces[1]\n",
    "        replicate = count_header_pieces[2]\n",
    "        result.append(count_header_pieces)\n",
    "\n",
    "        # fill out structure {some_id: {timept: {set of replicates}}} for use in\n",
    "        # validation after all columns are examined\n",
    "        if some_id not in expt_structure_by_id: expt_structure_by_id[some_id] = {}\n",
    "        curr_expt_structure = expt_structure_by_id[some_id]\n",
    "        if timept not in curr_expt_structure: curr_expt_structure[timept] = set()\n",
    "        curr_timept_replicates = curr_expt_structure[timept]\n",
    "        curr_timept_replicates.add(replicate)\n",
    "\n",
    "    _validate_expt_structure(expt_structure_by_id)\n",
    "    return result\n",
    "\n",
    "\n",
    "# So ... I wrote and tested _validate_expt_structure anticipating there could be\n",
    "# MORE than ONE experiment in the input file.  Now I think that can't be the\n",
    "# case.  However, it was a pain to write so in case I'm wrong, I'm leaving the\n",
    "# version that handles that complexity here for now.\n",
    "#\n",
    "# def _validate_expt_structure(expt_structure_by_id):\n",
    "#     # expt_structure_by_id should have format {some_id: {timept: {set of replicates}}}\n",
    "#\n",
    "#     # Every uid must have the exact same set of timepoints\n",
    "#     # e.g., can't have sample1_T1_1, sample1_T2_1; sample2_T1_1, sample2_T2_1, sample2_T3_1\n",
    "#     # Put all timepoints+replicates for a given sample in a set:\n",
    "#     # set {T1_1, T2_1} doesn't match set {T1_1, T2_1, T3_1}\n",
    "#\n",
    "#     # All timepoints (across all expts) must have the exact same set of replicates.\n",
    "#     # If timepts in *different* expts have different replicates, that would be\n",
    "#     # caught by the above comparison of timept+replicate sets across samples.\n",
    "#     # However, if timepts in the *same* sample have different numbers of replicates,\n",
    "#     # that wouldn't be caught unless we keep sets by timept rather than timept+replicate.\n",
    "#     # e.g., can't have sample1_T1_1; sample1_T2_1, sample1_T2_2\n",
    "#\n",
    "#     is_first_expt = True\n",
    "#     reference_timepts_plus_reps_set = None\n",
    "#\n",
    "#     for curr_expt_id, curr_expt_structure in expt_structure_by_id.items():\n",
    "#         # ensure all timepoints for current sample have the same number of replicates\n",
    "#         is_first_timept = True\n",
    "#         reference_reps_set = None\n",
    "#\n",
    "#         for curr_timept, curr_rep_set in curr_expt_structure.items():\n",
    "#             if is_first_timept:\n",
    "#                 reference_reps_set = curr_rep_set\n",
    "#                 is_first_timept = False\n",
    "#             else:\n",
    "#                 if curr_rep_set != reference_reps_set:\n",
    "#                     raise ValueError(\"For sample '{0}', timepoint {1} has \"\n",
    "#                                      \"replicates '{2}' instead of the expected '{3}'\".format(\n",
    "#                         curr_expt_id, curr_timept, sorted(curr_rep_set),\n",
    "#                         sorted(reference_reps_set)))\n",
    "#\n",
    "#             # make a new list of timept+rep for this timept\n",
    "#\n",
    "#\n",
    "#         # TODO: handle case where no timepts exist, or no reps exist for timept\n",
    "#         curr_timepts_plus_reps_set = {\"{0}_{1}\".format(timept, rep)\n",
    "#                                  for timept in curr_expt_structure\n",
    "#                                  for rep in reference_reps_set}\n",
    "#\n",
    "#         if is_first_expt:\n",
    "#             reference_timepts_plus_reps_set = curr_timepts_plus_reps_set\n",
    "#             is_first_expt = False\n",
    "#         else:\n",
    "#             if curr_timepts_plus_reps_set != reference_timepts_plus_reps_set:\n",
    "#                 raise ValueError(\"Sample {0} has timepoints+replicates \"\n",
    "#                                  \"'{1}' instead of the expected '{2}'\".format(\n",
    "#                     curr_expt_id, sorted(curr_timepts_plus_reps_set),\n",
    "#                     sorted(reference_timepts_plus_reps_set)))\n",
    "#\n",
    "#\n",
    "# def validate_data_column_headers(count_headers):\n",
    "#     # ensure that every timepoint in an experiment has the exact same set of replicates\n",
    "#     # ensure that every experiment has the exact same set of timepoints+replicates\n",
    "#\n",
    "#     expt_structure_by_id = {}\n",
    "#     for curr_count_header in count_headers:\n",
    "#         # Required count header format: experiment_timept_rep\n",
    "#         count_header_pieces = _validate_and_decompose_count_header(curr_count_header)\n",
    "#         some_id = count_header_pieces[0]\n",
    "#         timept = _validate_and_standardize_timepoint(count_header_pieces[1])\n",
    "#         replicate = count_header_pieces[2]\n",
    "#\n",
    "#         # fill out structure {some_id: {timept: {set of replicates}}} for use in\n",
    "#         # validation after all columns are examined\n",
    "#         if some_id not in expt_structure_by_id: expt_structure_by_id[some_id] = {}\n",
    "#         curr_expt_structure = expt_structure_by_id[some_id]\n",
    "#         if timept not in curr_expt_structure: curr_expt_structure[timept] = set()\n",
    "#         curr_timept_replicates = curr_expt_structure[timept]\n",
    "#         curr_timept_replicates.add(replicate)\n",
    "#\n",
    "#     _validate_expt_structure(expt_structure_by_id)\n",
    "\n",
    "\n",
    "def _generate_scoring_friendly_annotation(annotation_df):\n",
    "    construct_id_header = get_construct_header()\n",
    "    target_pair_id_header = get_target_pair_id_header()\n",
    "    probe_pair_id_header = get_probe_pair_id_header()\n",
    "\n",
    "    divider = get_header_divider()\n",
    "\n",
    "    result = annotation_df.loc[:, (construct_id_header, get_target_id_header(\"a\"),\n",
    "        get_probe_id_header(\"a\"), get_target_id_header(\"b\"), get_probe_id_header(\"b\"))]\n",
    "    target_pairs = (result[get_target_id_header(\"a\")] + divider +\n",
    "                    result[get_target_id_header(\"b\")])\n",
    "    probe_pairs = (result[get_probe_id_header(\"a\")] + divider + divider +\n",
    "                   result[get_probe_id_header(\"b\")])\n",
    "    add_series_to_dataframe(result, target_pairs, target_pair_id_header)\n",
    "    add_series_to_dataframe(result, probe_pairs, probe_pair_id_header)\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_and_recompose_count_header(expt_timeptnum_rep_tuple):\n",
    "    standardized_pieces = _validate_and_standardize_count_header_pieces(expt_timeptnum_rep_tuple)\n",
    "\n",
    "    divider = get_header_divider()\n",
    "    result = \"{}{}{}{}{}{}\".format(standardized_pieces[0], divider, get_time_prefix(), standardized_pieces[1],\n",
    "                divider, standardized_pieces[2])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ccbbucsd.utilities.pandas_utils import add_series_to_dataframe, merge_files_by_shared_header\n",
    "\n",
    "def load_annotation_df(constructs_fp, column_indices, disregard_order):\n",
    "    construct_df = _read_in_construct_table(constructs_fp, column_indices, rows_to_skip=1)\n",
    "    if disregard_order:\n",
    "        construct_df = _alphabetize_two_fields_in_row(construct_df, \n",
    "            get_target_id_header(\"a\"), get_target_id_header(\"b\"))\n",
    "        construct_df = _alphabetize_two_fields_in_row(construct_df, \n",
    "            get_probe_id_header(\"a\"), get_probe_id_header(\"b\"))\n",
    "    return construct_df  \n",
    "\n",
    "\n",
    "def _recompose_headers_from_tuples(header_pieces_tuples_list, sort=False):\n",
    "    input_list = sorted(header_pieces_tuples_list) if sort else header_pieces_tuples_list\n",
    "    return [validate_and_recompose_count_header(x) for x in input_list]\n",
    "\n",
    "\n",
    "def _validate_and_rename_counts_columns(combined_counts_df):\n",
    "    data_col_headers = list(combined_counts_df.columns.values)\n",
    "    data_col_headers.remove(get_construct_header())\n",
    "    \n",
    "    result = validate_and_parse_data_column_headers(data_col_headers)\n",
    "    \n",
    "    # recompose headers rather than using data_col_headers because their components\n",
    "    # may have been changed by standardization done in validate_and_parse_data_column_headers\n",
    "    unsorted_headers_list = _recompose_headers_from_tuples(result)\n",
    "    rename_dictionary = dict(zip(data_col_headers, unsorted_headers_list))\n",
    "    combined_counts_df.rename(columns=rename_dictionary, inplace=True)   \n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "def _get_sorted_joined_df_column_headers(minimal_annotation_df, header_pieces_tuples_list):\n",
    "    result = list(minimal_annotation_df.columns.values)\n",
    "    sorted_data_headers = _recompose_headers_from_tuples(header_pieces_tuples_list, True)\n",
    "    result.extend(sorted_data_headers)    \n",
    "    return result\n",
    "\n",
    "\n",
    "def merge_and_annotate_counts(count_file_fps, constructs_fp, column_indices, disregard_order=True):   \n",
    "    construct_id_header = get_construct_header()\n",
    "    \n",
    "    # load and validate the counts file(s)\n",
    "    combined_counts_df = merge_files_by_shared_header(count_file_fps, construct_id_header)\n",
    "    header_pieces_tuples_list = _validate_and_rename_counts_columns(combined_counts_df)\n",
    "    expt_name = header_pieces_tuples_list[0][0]\n",
    "      \n",
    "    # load and standardize the annotation file (containing construct definitions)\n",
    "    annotation_df = load_annotation_df(constructs_fp, column_indices, disregard_order)\n",
    "    minimal_annotation_df = _generate_scoring_friendly_annotation(annotation_df)\n",
    "    \n",
    "    # join counts to annotation and sort into required order\n",
    "    joined_df = minimal_annotation_df.merge(combined_counts_df, on=construct_id_header) \n",
    "    sorted_col_headers = _get_sorted_joined_df_column_headers(minimal_annotation_df,\n",
    "        header_pieces_tuples_list)\n",
    "    return expt_name, joined_df.loc[:, sorted_col_headers]    \n",
    "\n",
    "\n",
    "def merge_and_write_timepoint_counts(count_file_fps, constructs_fp, column_indices, disregard_order=True):   \n",
    "    expt_name, joined_df = merge_and_annotate_counts(count_file_fps, constructs_fp, \n",
    "        column_indices, disregard_order=True)\n",
    "    output_fp = os.path.join(g_scoring_ready_counts_dir, \n",
    "        \"{0}_timepoint_counts.csv\".format(expt_name))\n",
    "    joined_df.to_csv(output_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_and_write_timepoint_counts(g_count_file_fps, g_constructs_fp, g_col_indices, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
