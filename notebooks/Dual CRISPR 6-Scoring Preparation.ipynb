{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual CRISPR Screen Analysis\n",
    "# Step 6: Scoring Preparation\n",
    "Amanda Birmingham, CCBB, UCSD (abirmingham@ucsd.edu)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To run this notebook reproducibly, follow these steps:\n",
    "1. Click **Kernel** > **Restart & Clear Output**\n",
    "2. When prompted, click the red **Restart & clear all outputs** button\n",
    "3. Fill in the values for your analysis for each of the variables in the [Input Parameters](#Input-Parameters) section\n",
    "4. Click **Cell** > **Run All**\n",
    "\n",
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_dataset_name = \"smallNotebookTest\"\n",
    "g_library_fp = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/CV4_2spacers_w_probe_names_wo_duplicate.txt')\n",
    "g_count_fps_or_dirs = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/notebook4_small_notebook_test')\n",
    "g_prepped_counts_run_prefix = \"test_small_notebook\"\n",
    "g_prepped_counts_dir = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/notebook6_small_notebook_test')\n",
    "g_time_prefixes = \"T,D\"\n",
    "g_code_location = '/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sys\n",
    "sys.path.append(g_code_location)\n",
    "\n",
    "import ccbbucsd.utilities.analysis_run_prefixes as ns_runs\n",
    "import ccbbucsd.utilities.files_and_paths as ns_files\n",
    "import ccbbucsd.utilities.notebook_logging as ns_logs\n",
    "\n",
    "\n",
    "def describe_var_list(input_var_name_list):\n",
    "    description_list =  [\"{0}: {1}\\n\".format(name, eval(name)) for name in input_var_name_list]\n",
    "    return \"\".join(description_list)\n",
    "\n",
    "\n",
    "ns_logs.set_stdout_info_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_combined_counts_file_suffix():\n",
      "    return \"counts_combined.txt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.count_combination as ns_combine\n",
    "print(inspect.getsource(ns_combine.get_combined_counts_file_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def split_delimited_string_to_list(input_str, cast_func=str, delimiter_str=\",\"):\n",
      "    return [cast_func(x.strip()) for x in input_str.split(delimiter_str)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.utilities.string_utils as ns_string\n",
    "print(inspect.getsource(ns_string.split_delimited_string_to_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_count_file_fps: ['/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/test_files/small_notebook_test/notebook4_small_notebook_test/test_small_notebook_counts_combined.txt']\n",
      "g_prepped_counts_run_prefix: test_small_notebook\n",
      "g_time_prefixes_list: ['T', 'D']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "def get_count_file_fps(comma_sep_fps_or_dirs_str):\n",
    "    result = []\n",
    "    \n",
    "    fps_or_dirs = comma_sep_fps_or_dirs_str.split(\",\")\n",
    "    for curr_fp_or_dir in fps_or_dirs:\n",
    "        trimmed_curr = curr_fp_or_dir.strip()\n",
    "        if os.path.isdir(trimmed_curr):\n",
    "            combined_counts_fps = ns_files.get_filepaths_from_wildcard(trimmed_curr, \n",
    "                ns_combine.get_combined_counts_file_suffix())    \n",
    "            result.extend(combined_counts_fps)\n",
    "        else:\n",
    "            result.append(trimmed_curr)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "\n",
    "g_count_file_fps = get_count_file_fps(g_count_fps_or_dirs)\n",
    "g_prepped_counts_run_prefix = ns_runs.check_or_set(g_prepped_counts_run_prefix,\n",
    "                                                   ns_runs.generate_run_prefix(g_dataset_name))\n",
    "g_time_prefixes_list = ns_string.split_delimited_string_to_list(g_time_prefixes)\n",
    "print(describe_var_list(['g_count_file_fps', 'g_prepped_counts_run_prefix', 'g_time_prefixes_list']))\n",
    "ns_files.verify_or_make_dir(g_prepped_counts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring-Ready File Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ccbb libraries\n",
      "import ccbbucsd.utilities.pandas_utils as ns_pandas\n",
      "\n",
      "# project-specific libraries\n",
      "import ccbbucsd.malicrispr.construct_file_extracter as ns_extracter\n",
      "import ccbbucsd.malicrispr.count_files_and_dataframes as ns_count\n",
      "\n",
      "\n",
      "__author__ = \"Amanda Birmingham\"\n",
      "__maintainer__ = \"Amanda Birmingham\"\n",
      "__email__ = \"abirmingham@ucsd.edu\"\n",
      "__status__ = \"development\"\n",
      "\n",
      "\n",
      "def get_prepped_file_suffix():\n",
      "    return \"timepoint_counts.txt\"\n",
      "\n",
      "\n",
      "def get_sample_name_header():\n",
      "    return \"sampleName\"\n",
      "\n",
      "\n",
      "def get_abundance_thresh_header():\n",
      "    return \"log2CountsThresh\"\n",
      "\n",
      "\n",
      "def get_abundance_thresh_file_suffix():\n",
      "    return \"abundance_thresholds.txt\"\n",
      "\n",
      "\n",
      "def read_timepoint_from_standardized_count_header(count_header, time_prefixes_list):\n",
      "    count_header_pieces = count_header.split(ns_extracter.get_header_divider())\n",
      "    if len(count_header_pieces) != _get_num_header_pieces() + 1:\n",
      "        # +1 because we expect a standardized header, which has the experiment id added to it\n",
      "        raise ValueError(\"Column header '{0}' splits into an unexpected number of pieces ({1})\".format(\n",
      "            count_header, len(count_header_pieces)\n",
      "        ))\n",
      "\n",
      "    timepoint_str = count_header_pieces[_get_timepoint_index()]\n",
      "    return _validate_and_standardize_timepoint(timepoint_str, time_prefixes_list)\n",
      "\n",
      "\n",
      "def merge_and_annotate_counts(count_file_fps, constructs_fp, dataset_name, time_prefixes_list,\n",
      "                              disregard_order=True):\n",
      "\n",
      "    construct_id_header = ns_extracter.get_construct_header()\n",
      "\n",
      "    # load and merge the counts file(s)\n",
      "    combined_counts_df = ns_pandas.merge_files_by_shared_header(count_file_fps, construct_id_header)\n",
      "\n",
      "    # validate, standardize, and rename the count column headers\n",
      "    orig_count_headers = _get_orig_count_headers(combined_counts_df)\n",
      "    standardized_count_headers = _validate_and_standardize_count_headers(orig_count_headers,\n",
      "                                                                         dataset_name, time_prefixes_list)\n",
      "    rename_dictionary = dict(zip(orig_count_headers, standardized_count_headers))\n",
      "    combined_counts_df.rename(columns=rename_dictionary, inplace=True)\n",
      "\n",
      "    # load and standardize the annotation file (containing construct definitions)\n",
      "    annotation_df = ns_extracter.load_annotation_df(constructs_fp, disregard_order)\n",
      "    minimal_annotation_df = _generate_scoring_friendly_annotation(annotation_df)\n",
      "\n",
      "    # join counts to annotation and sort into required order\n",
      "    joined_df = minimal_annotation_df.merge(combined_counts_df, on=construct_id_header)\n",
      "    sorted_col_headers = list(minimal_annotation_df.columns.values)\n",
      "    sorted_col_headers.extend(sorted(standardized_count_headers))\n",
      "    return joined_df.loc[:, sorted_col_headers]\n",
      "\n",
      "\n",
      "def _get_timepoint_index():\n",
      "    return -2\n",
      "\n",
      "\n",
      "def _get_num_header_pieces():\n",
      "    return 2\n",
      "\n",
      "\n",
      "def _get_preferred_timept_prefix(time_prefixes_list):\n",
      "    return time_prefixes_list[0]\n",
      "\n",
      "\n",
      "def _get_orig_count_headers(combined_counts_df):\n",
      "    orig_count_headers = list(combined_counts_df.columns.values)\n",
      "    orig_count_headers.remove(ns_extracter.get_construct_header())\n",
      "    return orig_count_headers\n",
      "\n",
      "\n",
      "def _validate_and_standardize_count_headers(orig_count_headers, expt_id, time_prefixes_list):\n",
      "    expt_structure_by_id = {}\n",
      "    result = []\n",
      "\n",
      "    for curr_count_header in orig_count_headers:\n",
      "        # Required count header format: experiment_timept_rep\n",
      "        valid_id, timept, replicate = _validate_and_standardize_header_pieces(curr_count_header, expt_id,\n",
      "                                                                             time_prefixes_list)\n",
      "        standardized_header = _recompose_count_header(valid_id, timept, replicate, time_prefixes_list)\n",
      "        result.append(standardized_header)\n",
      "\n",
      "        # fill out structure {some_id: {timept: {set of replicates}}} for use in\n",
      "        # validation after all columns are examined\n",
      "        if valid_id not in expt_structure_by_id: expt_structure_by_id[valid_id] = {}\n",
      "        curr_expt_structure = expt_structure_by_id[valid_id]\n",
      "        if timept not in curr_expt_structure: curr_expt_structure[timept] = set()\n",
      "        curr_timept_replicates = curr_expt_structure[timept]\n",
      "        curr_timept_replicates.add(replicate)\n",
      "\n",
      "    _validate_expt_structure(expt_structure_by_id)\n",
      "    return result\n",
      "\n",
      "\n",
      "# this method is broken out from _validate_and_standardize_count_headers just to make unit testing easier\n",
      "def _validate_and_standardize_header_pieces(curr_count_header, expt_id, time_prefixes_list):\n",
      "    timept, replicate = _validate_and_standardize_timept_and_replicate(\n",
      "        curr_count_header, time_prefixes_list)\n",
      "    valid_id = _validate_and_standardize_expt_id(expt_id)\n",
      "\n",
      "    return valid_id, timept, replicate\n",
      "\n",
      "\n",
      "def _validate_and_standardize_timept_and_replicate(count_header, time_prefixes_list):\n",
      "    # Required count header format: experiment_timept_rep\n",
      "    trimmed_count_header = ns_count.clip_count_header_suffix(count_header)\n",
      "    count_header_pieces = trimmed_count_header.split(ns_extracter.get_header_divider())\n",
      "\n",
      "    num_expected_pieces = _get_num_header_pieces()\n",
      "    if len(count_header_pieces) < num_expected_pieces:\n",
      "        raise ValueError(\"Count header has fewer than the expected {0} piece(s): '{1}'.\".format(\n",
      "                         num_expected_pieces, count_header_pieces))\n",
      "\n",
      "    timept = _validate_and_standardize_timepoint(count_header_pieces[_get_timepoint_index()],\n",
      "                                                 time_prefixes_list)\n",
      "    replicate = _validate_and_standardize_replicate(count_header_pieces[-1])\n",
      "    return timept, replicate\n",
      "\n",
      "\n",
      "def _validate_and_standardize_timepoint(timept, time_prefixes_list):\n",
      "    if isinstance(timept, str):\n",
      "        # ensure timepoint is \"t\" or \"T\" plus a non-negative integer number\n",
      "        expected_timepoint_prefixes = [x.upper() for x in time_prefixes_list]\n",
      "        timepoint_prefix = timept[:1]\n",
      "        if timepoint_prefix.upper() not in expected_timepoint_prefixes:\n",
      "            raise ValueError(\"Time point '{0}' does not start with upper or lower case versions of any of the \"\n",
      "                             \"expected prefixes {1}.\".format(\n",
      "                             timept, \", \".join(time_prefixes_list)))\n",
      "\n",
      "        timept = timept[1:]\n",
      "    else:\n",
      "        timept = str(timept)\n",
      "\n",
      "    if not timept.isdigit():\n",
      "        raise ValueError(\"Time point value '{0}' is not recognizable as a positive integer.\".format(\n",
      "                         timept))\n",
      "\n",
      "    return int(timept)\n",
      "\n",
      "\n",
      "def _validate_and_standardize_replicate(rep):\n",
      "    if not isinstance(rep, int):\n",
      "        rep = int(rep) if rep.isdigit() else rep\n",
      "    return rep\n",
      "\n",
      "\n",
      "def _validate_and_standardize_expt_id(expt_id):\n",
      "    # experiment ids are used as a component of the sample names--and sample names are used as\n",
      "    # dataframe *column names* in both pandas (Python) and R.  In R, column names must be valid (R) variable\n",
      "    # names, and thus may contain only alphanumerics, periods, and underscores.  In pandas, column names\n",
      "    # must be valid Python variable names, which may contain only alphanumerics and underscores.  Thus,\n",
      "    # the only *non*-alphanumeric character that is accepted by both is the underscore, and I use that\n",
      "    # to delimit the pieces of the sample name (i.e., exptid_timept_replicatenum).  That means expt id\n",
      "    # can't contain any underscores itself--leaving only alphanumerics :(\n",
      "    if not expt_id.isalnum():\n",
      "        raise ValueError(\"Experiment id '{0}' is not strictly alphanumeric.\".format(\n",
      "                         expt_id))\n",
      "\n",
      "    return expt_id\n",
      "\n",
      "\n",
      "def _recompose_count_header(expt_id, timept, replicate, time_prefixes_list):\n",
      "    time_prefix = _get_preferred_timept_prefix(time_prefixes_list)\n",
      "    divider = ns_extracter.get_header_divider()\n",
      "    result = \"{}{}{}{}{}{}\".format(expt_id, divider, time_prefix,\n",
      "                                   timept, divider, replicate)\n",
      "    return result\n",
      "\n",
      "\n",
      "def _validate_expt_structure(expt_structure_by_id):\n",
      "    # expt_structure_by_id should have format {some_id: {timept: {set of replicates}}}\n",
      "\n",
      "    # There must be only one experiment represented in the data structure\n",
      "\n",
      "    # All timepoints in the experiment must have the exact same set of replicates:\n",
      "    # e.g., can't have sample1_T1_1; sample1_T2_1, sample1_T2_2\n",
      "\n",
      "    if len(expt_structure_by_id) != 1:\n",
      "        raise ValueError((\"Count headers must describe one and only one experiment, \"\n",
      "                          \"but {0} were detected: '{1}'.\").format(len(expt_structure_by_id),\n",
      "                                                                  sorted(list(expt_structure_by_id.keys()))))\n",
      "\n",
      "    for curr_expt_id, curr_expt_structure in expt_structure_by_id.items():\n",
      "        # ensure all timepoints for current sample have the same number of replicates\n",
      "        is_first_timept = True\n",
      "        reference_reps_set = None\n",
      "\n",
      "        if len(curr_expt_structure) == 0:\n",
      "            raise ValueError(\"Count headers must describe at least one timepoint for experiment, \"\n",
      "                             \"but 0 were detected.\")\n",
      "\n",
      "        for curr_timept, curr_rep_set in curr_expt_structure.items():\n",
      "            if len(curr_rep_set) == 0:\n",
      "                raise ValueError((\"Count headers must describe at least one replicate for each timepoint, \"\n",
      "                                  \"but 0 were detected for timepoint '{0}'.\").format(curr_timept))\n",
      "\n",
      "            if is_first_timept:\n",
      "                reference_reps_set = curr_rep_set\n",
      "                is_first_timept = False\n",
      "            else:\n",
      "                if curr_rep_set != reference_reps_set:\n",
      "                    raise ValueError(\"For sample '{0}', timepoint {1} has \"\n",
      "                                     \"replicates '{2}' instead of the expected '{3}'\".format(\n",
      "                        curr_expt_id, curr_timept, sorted(curr_rep_set),\n",
      "                        sorted(reference_reps_set)))\n",
      "\n",
      "\n",
      "def _generate_scoring_friendly_annotation(annotation_df):\n",
      "    construct_id_header = ns_extracter.get_construct_header()\n",
      "\n",
      "    result = annotation_df.loc[:, [construct_id_header,\n",
      "                                   ns_extracter.get_probe_id_header(\"a\"),\n",
      "                                   ns_extracter.get_probe_id_header(\"b\"),\n",
      "                                   ns_extracter.get_target_id_header(\"a\"),\n",
      "                                   ns_extracter.get_target_id_header(\"b\")\n",
      "                                   ]]\n",
      "\n",
      "    # Below is what I expect the output to be after scoring data prep code is refactored to accept more\n",
      "    # detail (and generate less itself).\n",
      "    # result = annotation_df.loc[:, (construct_id_header, ns_extracter.get_target_id_header(\"a\"),\n",
      "    #                                ns_extracter.get_probe_id_header(\"a\"),\n",
      "    #                                ns_extracter.get_target_id_header(\"b\"),\n",
      "    #                                ns_extracter.get_probe_id_header(\"b\"))]\n",
      "    #target_pair_id_header = ns_extracter.get_target_pair_id_header()\n",
      "    #probe_pair_id_header = ns_extracter.get_probe_pair_id_header()\n",
      "    # Note: the below column creations could be done without using apply (i.e., by\n",
      "    # just writing \"df[colA] + divider + df[colB]\") but I used apply because I want\n",
      "    # to centralize the code that creates these strings, and sometimes it needs to work\n",
      "    # on a single pair of variables rather than columns of variables, so it needed to be\n",
      "    # a non-vectorized function.\n",
      "    #result[target_pair_id_header] = result.apply(_compose_target_pair_id, axis=1)\n",
      "    #result[probe_pair_id_header] = result.apply(_compose_probe_pair_id, axis=1)\n",
      "    return result\n",
      "\n",
      "\n",
      "# def _compose_probe_pair_id(row):\n",
      "#     return ns_extracter.compose_probe_pair_id_from_probe_ids(row[ns_extracter.get_probe_id_header(\"a\")],\n",
      "#                                                 row[ns_extracter.get_probe_id_header(\"b\")])\n",
      "#\n",
      "#\n",
      "# def _compose_target_pair_id(row):\n",
      "#     return ns_extracter.compose_target_pair_id_from_target_ids(row[ns_extracter.get_target_id_header(\"a\")],\n",
      "#                                                   row[ns_extracter.get_target_id_header(\"b\")])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.scoring_prep as ns_prep\n",
    "print(inspect.getsource(ns_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_and_write_timepoint_counts(count_file_fps, constructs_fp, run_prefix, dataset_name, time_prefixes_list,\n",
    "                                     output_dir, disregard_order=True):   \n",
    "    \n",
    "    joined_df = ns_prep.merge_and_annotate_counts(count_file_fps, constructs_fp, dataset_name, \n",
    "        time_prefixes_list, disregard_order=True)\n",
    "    prepped_file_suffix = ns_prep.get_prepped_file_suffix()\n",
    "    output_fp = ns_files.build_multipart_fp(output_dir, [run_prefix, prepped_file_suffix])\n",
    "    joined_df.to_csv(output_fp, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_and_write_timepoint_counts(g_count_file_fps, g_library_fp, g_prepped_counts_run_prefix, g_dataset_name,\n",
    "                                 g_time_prefixes_list, g_prepped_counts_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
