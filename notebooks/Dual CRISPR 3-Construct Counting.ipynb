{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual CRISPR Screen Analysis\n",
    "# Step 3: Construct Counting\n",
    "Amanda Birmingham, CCBB, UCSD (abirmingham@ucsd.edu)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To run this notebook reproducibly, follow these steps:\n",
    "1. Click **Kernel** > **Restart & Clear Output**\n",
    "2. When prompted, click the red **Restart & clear all outputs** button\n",
    "3. Fill in the values for your analysis for each of the variables in the [Input Parameters](#Input-Parameters) section\n",
    "4. Click **Cell** > **Run All**\n",
    "\n",
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_num_processors = 3\n",
    "g_filtered_fastqs_dir = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/notebook2_small_notebook_test')\n",
    "g_library_fp = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/CV4_2spacers_w_probe_names_wo_duplicate.txt')\n",
    "g_len_of_seq_to_match = 19\n",
    "g_num_allowed_mismatches = 1\n",
    "g_fastq_counts_run_prefix = 'test_small_notebook'\n",
    "g_fastq_counts_dir = ('/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'\n",
    "    'test_files/small_notebook_test/notebook3_small_notebook_test')\n",
    "g_code_location = '/Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sys\n",
    "sys.path.append(g_code_location)\n",
    "\n",
    "import ccbbucsd.utilities.analysis_run_prefixes as ns_runs\n",
    "import ccbbucsd.utilities.files_and_paths as ns_files\n",
    "import ccbbucsd.utilities.notebook_logging as ns_logs\n",
    "\n",
    "\n",
    "def describe_var_list(input_var_name_list):\n",
    "    description_list =  [\"{0}: {1}\\n\".format(name, eval(name)) for name in input_var_name_list]\n",
    "    return \"\".join(description_list)\n",
    "\n",
    "\n",
    "ns_logs.set_stdout_info_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_fastq_counts_dir: /Users/Birmingham/Work/Repositories/ccbb_tickets_2017/mali-dual-crispr-pipeline/src/python/test_files/small_notebook_test/notebook3_small_notebook_test\n",
      "g_fastq_counts_run_prefix: test_small_notebook\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_fastq_counts_dir = ns_runs.check_or_set(g_fastq_counts_dir, g_filtered_fastqs_dir)\n",
    "g_fastq_counts_run_prefix = ns_runs.check_or_set(g_fastq_counts_run_prefix, ns_runs.generate_run_prefix())\n",
    "print(describe_var_list(['g_fastq_counts_run_prefix', 'g_fastq_counts_dir']))\n",
    "ns_files.verify_or_make_dir(g_fastq_counts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Counting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_filtered_file_suffix():\n",
      "    return \"_len_filtered.fastq\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.count_filterer as ns_filter\n",
    "print(inspect.getsource(ns_filter.get_filtered_file_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# third-party libraries\n",
      "import pandas\n",
      "\n",
      "# ccbb libraries\n",
      "from ccbbucsd.utilities.bio_seq_utilities import trim_seq\n",
      "\n",
      "__author__ = \"Amanda Birmingham\"\n",
      "__maintainer__ = \"Amanda Birmingham\"\n",
      "__email__ = \"abirmingham@ucsd.edu\"\n",
      "__status__ = \"prototype\"\n",
      "\n",
      "_CONSTRUCT_ID = \"construct_id\"\n",
      "_PROBE_A_SEQ = \"probe_a_seq\"\n",
      "_PROBE_B_SEQ = \"probe_b_seq\"\n",
      "_PROBE_A_NAME = \"probe_a_id\"\n",
      "_PROBE_B_NAME = \"probe_b_id\"\n",
      "_TARGET_A_NAME = \"target_a_id\"\n",
      "_TARGET_B_NAME = \"target_b_id\"\n",
      "_TARGET_PAIR_ID = \"target_pair_id\"\n",
      "_PROBE_PAIR_ID = \"probe_pair_id\"\n",
      "_HEADER_DIVIDER = \"_\"\n",
      "\n",
      "\n",
      "def get_potential_annotation_headers():\n",
      "    return [_CONSTRUCT_ID, _PROBE_A_SEQ, _PROBE_B_SEQ, _PROBE_A_NAME, _PROBE_B_NAME, _TARGET_A_NAME, _TARGET_B_NAME,\n",
      "            _TARGET_PAIR_ID, _PROBE_PAIR_ID]\n",
      "\n",
      "\n",
      "def get_header_divider():\n",
      "    return _HEADER_DIVIDER\n",
      "\n",
      "\n",
      "def get_construct_header():\n",
      "    return _CONSTRUCT_ID\n",
      "\n",
      "\n",
      "def get_probe_id_header(probe_letter):\n",
      "    return _PROBE_A_NAME if _is_letter_a(probe_letter) else _PROBE_B_NAME\n",
      "\n",
      "\n",
      "def get_probe_seq_header(probe_letter):\n",
      "    return _PROBE_A_SEQ if _is_letter_a(probe_letter) else _PROBE_B_SEQ\n",
      "\n",
      "\n",
      "def get_target_id_header(target_letter):\n",
      "    return _TARGET_A_NAME if _is_letter_a(target_letter) else _TARGET_B_NAME\n",
      "\n",
      "\n",
      "def get_target_pair_id_header():\n",
      "    return _TARGET_PAIR_ID\n",
      "\n",
      "\n",
      "def get_probe_pair_id_header():\n",
      "    return _PROBE_PAIR_ID\n",
      "\n",
      "\n",
      "def get_comment_char():\n",
      "    return \"#\"\n",
      "\n",
      "\n",
      "def compose_probe_pair_id_from_probe_ids(probe_a_id, probe_b_id):\n",
      "    divider = get_header_divider()\n",
      "    result = probe_a_id + divider + divider + probe_b_id\n",
      "    return result\n",
      "\n",
      "\n",
      "def compose_target_pair_id_from_target_ids(target_a_id, target_b_id):\n",
      "    return target_a_id + get_header_divider() + target_b_id\n",
      "\n",
      "\n",
      "def extract_construct_and_grna_info(constructs_fp):\n",
      "    construct_table = _read_in_construct_table(constructs_fp)\n",
      "    seq_name_sets = _extract_unique_sets_across_a_and_b(construct_table,\n",
      "        [_PROBE_A_NAME, _PROBE_A_SEQ], [_PROBE_B_NAME, _PROBE_B_SEQ])\n",
      "    probe_name_seq_pairs = _validate_and_format_probe_seq_pairs(seq_name_sets)\n",
      "    construct_names = construct_table[_CONSTRUCT_ID].unique().tolist()\n",
      "    return construct_names, probe_name_seq_pairs\n",
      "\n",
      "\n",
      "def trim_probes(probes_name_and_seq_list, retain_len):\n",
      "    result = []\n",
      "    for name_seq_tuple in probes_name_and_seq_list:\n",
      "        probe_name = name_seq_tuple[0]\n",
      "        full_seq = name_seq_tuple[1]\n",
      "        trimmed_seq = trim_seq(full_seq, retain_len, False)  # False = do not retain from 5p end but from 3p end\n",
      "        result.append((probe_name, trimmed_seq))\n",
      "    return result\n",
      "\n",
      "\n",
      "def load_annotation_df(constructs_fp, disregard_order):\n",
      "    construct_df = _read_in_construct_table(constructs_fp)\n",
      "\n",
      "    # TODO: I'm not thrilled that if there is non-uniqueness, I don't learn about it till this call;\n",
      "    # I'd sort of like to learn about it from extract_construct_and_grna_info, even though that\n",
      "    # method is able to successfully work around non-unique construct ids ...\n",
      "    if len(construct_df[_CONSTRUCT_ID].unique()) != len(construct_df[_CONSTRUCT_ID]):\n",
      "        raise ValueError(\"Non-unique construct ids detected.\")\n",
      "\n",
      "    if disregard_order:\n",
      "        construct_df = _alphabetize_two_fields_in_row(construct_df,\n",
      "                                                      get_target_id_header(\"a\"), get_target_id_header(\"b\"))\n",
      "        construct_df = _alphabetize_two_fields_in_row(construct_df,\n",
      "                                                      get_probe_id_header(\"a\"), get_probe_id_header(\"b\"))\n",
      "    return construct_df\n",
      "\n",
      "\n",
      "def _is_letter_a(letter):\n",
      "    if letter.upper() == \"A\":\n",
      "        result = True\n",
      "    elif letter.upper() == \"B\":\n",
      "        result = False\n",
      "    else:\n",
      "        raise ValueError(\"Input '{0}' is not recognized as A or B.\".format(letter))\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def _read_in_construct_table(constructs_fp, rows_to_skip=4):\n",
      "    result = pandas.read_table(constructs_fp, comment=get_comment_char(), skiprows=rows_to_skip, header=None)\n",
      "    result = _rename_columns(result)\n",
      "    return result\n",
      "\n",
      "\n",
      "def _rename_columns(construct_table, column_indices=None):\n",
      "    new_names = [_CONSTRUCT_ID, _TARGET_A_NAME, _PROBE_A_NAME, _PROBE_A_SEQ,\n",
      "                 _TARGET_B_NAME, _PROBE_B_NAME, _PROBE_B_SEQ]\n",
      "    existing_names = list(construct_table.columns.values)\n",
      "    if column_indices is None:\n",
      "        column_indices = range(0, len(new_names))\n",
      "\n",
      "    if len(column_indices) != len(new_names):\n",
      "        raise ValueError(\"Expected indices for {0} columns but received indices for {1}.\".format(\n",
      "            len(new_names), len(column_indices)))\n",
      "\n",
      "    existing_to_new_names = {}\n",
      "    for curr_index in range(0, len(column_indices)):\n",
      "        curr_col_index = column_indices[curr_index]\n",
      "        curr_existing_name = existing_names[curr_col_index]\n",
      "        existing_to_new_names[curr_existing_name] = new_names[curr_index]\n",
      "\n",
      "    return construct_table.rename(columns=existing_to_new_names)\n",
      "\n",
      "\n",
      "def _extract_unique_sets_across_a_and_b(construct_table, a_col_headers_list, b_col_headers_list):\n",
      "    if len(a_col_headers_list) != len(b_col_headers_list):\n",
      "        raise ValueError(\"A and B column header lists are not equal in length.\")\n",
      "\n",
      "    new_headers_list = [\"temp_header_{0}\".format(i) for i in range(0,len(a_col_headers_list))]\n",
      "\n",
      "    # get the set of input columns for each of the two targets, assigning each the same\n",
      "    # set of (generic) column headers so that they can easily be concatenated\n",
      "    set_for_a = _extract_renamed_subset_df(construct_table, a_col_headers_list, new_headers_list)\n",
      "    set_for_b = _extract_renamed_subset_df(construct_table, b_col_headers_list, new_headers_list)\n",
      "    combined_set = pandas.concat([set_for_a, set_for_b])\n",
      "\n",
      "    # extract only the unique sets\n",
      "    grouped_combined_set = combined_set.groupby(new_headers_list).groups\n",
      "    result = [x for x in grouped_combined_set]\n",
      "    return sorted(result)  # NB sort so that output order is predictable\n",
      "\n",
      "\n",
      "def _extract_renamed_subset_df(construct_table, col_headers_list, new_headers_list):\n",
      "    result = construct_table.loc[:, col_headers_list]\n",
      "    rename_dictionary = dict(zip(col_headers_list, new_headers_list))\n",
      "    result.rename(columns=rename_dictionary, inplace=True)\n",
      "    return result\n",
      "\n",
      "\n",
      "def _validate_and_format_probe_seq_pairs(probes_seq_and_name_list):\n",
      "    expected_num_pieces = 2\n",
      "    seqs_by_names = {}\n",
      "    names_by_seqs = {}\n",
      "    result = []\n",
      "\n",
      "    for curr_set in probes_seq_and_name_list:\n",
      "        if len(curr_set) != expected_num_pieces:\n",
      "            raise ValueError(\n",
      "                \"input '{0}' has {1} pieces instead of the expected {2}\".format(\n",
      "                    curr_set, len(curr_set), expected_num_pieces\n",
      "                ))\n",
      "        curr_seq = curr_set[1]\n",
      "        curr_name = curr_set[0]\n",
      "\n",
      "        if curr_seq in names_by_seqs:\n",
      "            raise ValueError(\n",
      "                \"sequence '{0}' associated with name '{1}' but was already associated with name '{2}'\".format(\n",
      "                    curr_seq, curr_name, names_by_seqs[curr_seq]\n",
      "                ))\n",
      "\n",
      "        if curr_name in seqs_by_names:\n",
      "            raise ValueError(\n",
      "                \"name '{0}' associated with sequence '{1}' but was already associated with sequence '{2}'\".format(\n",
      "                    curr_name, curr_seq, seqs_by_names[curr_name]\n",
      "                ))\n",
      "\n",
      "        names_by_seqs[curr_seq] = curr_name\n",
      "        seqs_by_names[curr_name] = curr_seq\n",
      "\n",
      "        result.append((curr_name, curr_seq.upper())) # upper-case all probe seqs\n",
      "    # next pair in\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def _alphabetize_two_fields_in_row(input_df, header_of_col_to_be_first, header_of_col_to_be_second):\n",
      "    is_row_dealphabetized = input_df[header_of_col_to_be_first] >input_df[header_of_col_to_be_second] # boolean array\n",
      "\n",
      "    # for dealphabetized rows, get header_of_col_to_be_first's value\n",
      "    orig_first_col_vals_for_out_of_order_rows = input_df.loc[is_row_dealphabetized, header_of_col_to_be_first]\n",
      "\n",
      "    # replace header_of_col_to_be_first's value with header_of_col_to_be_second's value for dealphabetized rows\n",
      "    input_df.loc[is_row_dealphabetized, header_of_col_to_be_first] = input_df.loc[is_row_dealphabetized,\n",
      "                                                                                header_of_col_to_be_second]\n",
      "    # replace header_of_col_to_be_second's value with header_of_col_to_be_first's original value for dealphabetized rows\n",
      "    input_df.loc[is_row_dealphabetized, header_of_col_to_be_second] = orig_first_col_vals_for_out_of_order_rows\n",
      "    return input_df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.construct_file_extracter as ns_extractor\n",
    "print(inspect.getsource(ns_extractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ccbb libraries\n",
      "from ccbbucsd.utilities.bio_seq_utilities import rev_comp_canonical_dna_seq\n",
      "\n",
      "__author__ = \"Amanda Birmingham\"\n",
      "__maintainer__ = \"Amanda Birmingham\"\n",
      "__email__ = \"abirmingham@ucsd.edu\"\n",
      "__status__ = \"development\"\n",
      "\n",
      "\n",
      "class GrnaPositionMatcher:\n",
      "    @staticmethod\n",
      "    def _generate_seqs_to_check(fw_whole_seq, rv_whole_seq):\n",
      "        rc_whole_rv_seq = rev_comp_canonical_dna_seq(rv_whole_seq)\n",
      "        return fw_whole_seq, rc_whole_rv_seq\n",
      "\n",
      "    def __init__(self, grna_names_and_seqs, expected_len, num_allowed_fw_mismatches, num_allowed_rv_mismatches):\n",
      "        self._grna_names_and_seqs = grna_names_and_seqs\n",
      "        self._num_allowed_fw_mismatches = num_allowed_fw_mismatches\n",
      "        self._num_allowed_rv_mismatches = num_allowed_rv_mismatches\n",
      "        self._seq_len = expected_len\n",
      "\n",
      "    @property\n",
      "    def num_allowed_fw_mismatches(self):\n",
      "        return self._num_allowed_fw_mismatches\n",
      "\n",
      "    @property\n",
      "    def num_allowed_rv_mismatches(self):\n",
      "        return self._num_allowed_rv_mismatches\n",
      "\n",
      "    def find_fw_and_rv_read_matches(self, fw_whole_seq, rv_whole_seq):\n",
      "        fw_construct_window, rc_rv_construct_window = self._generate_seqs_to_check(fw_whole_seq, rv_whole_seq)\n",
      "        return self._id_pair_matches(fw_construct_window, rc_rv_construct_window)\n",
      "\n",
      "    def _id_pair_matches(self, input1_seq, input2_seq):\n",
      "        input2_match_name = None\n",
      "        input1_match_name = self._id_sequence_match(self.num_allowed_fw_mismatches, input1_seq)\n",
      "        if input1_match_name is not None:\n",
      "            input2_match_name = self._id_sequence_match(self.num_allowed_rv_mismatches, input2_seq)\n",
      "\n",
      "        return input1_match_name, input2_match_name\n",
      "\n",
      "    def _id_sequence_match(self, num_allowed_mismatches, input_seq):\n",
      "        found_name = None\n",
      "\n",
      "        # check for perfect matches\n",
      "        for potential_name, potential_reference in self._grna_names_and_seqs:\n",
      "            if input_seq == potential_reference:\n",
      "                found_name = potential_name\n",
      "                break\n",
      "\n",
      "        # if no perfect matches found, check for mismatches\n",
      "        if found_name is None:\n",
      "            min_found_mismatches = num_allowed_mismatches + 1  # nothing checked yet so num mismatches is maximum\n",
      "            found_name = None\n",
      "            for potential_name, potential_reference in self._grna_names_and_seqs:\n",
      "\n",
      "                # this way is not elegant, but it IS fast :) .... >3x faster than XOR approach\n",
      "                num_mismatches = 0\n",
      "                for x in range(0, self._seq_len):\n",
      "                    if input_seq[x] != potential_reference[x]:  # NB assumption that both are the same fixed length!\n",
      "                        num_mismatches += 1\n",
      "                        if num_mismatches > num_allowed_mismatches:\n",
      "                            break\n",
      "\n",
      "                if num_mismatches < min_found_mismatches:\n",
      "                    min_found_mismatches = num_mismatches\n",
      "                    if num_mismatches <= num_allowed_mismatches:\n",
      "                        found_name = potential_name\n",
      "\n",
      "        return found_name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.grna_position_matcher as ns_matcher\n",
    "print(inspect.getsource(ns_matcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"This module counts almost-perfect matches of small sequences within forward and reverse fastq sequence pairs.\"\"\"\n",
      "\n",
      "# standard libraries\n",
      "import csv\n",
      "import datetime\n",
      "import logging\n",
      "\n",
      "# ccbb libraries\n",
      "from ccbbucsd.utilities.basic_fastq import FastqHandler, paired_fastq_generator\n",
      "\n",
      "# project-specific libraries\n",
      "from ccbbucsd.malicrispr.construct_file_extracter import compose_probe_pair_id_from_probe_ids\n",
      "\n",
      "__author__ = \"Amanda Birmingham\"\n",
      "__maintainer__ = \"Amanda Birmingham\"\n",
      "__email__ = \"abirmingham@ucsd.edu\"\n",
      "__status__ = \"development\"\n",
      "\n",
      "\n",
      "def get_counts_file_suffix():\n",
      "    return \"counts.txt\"\n",
      "\n",
      "\n",
      "def get_construct_header():\n",
      "    return \"construct_id\"\n",
      "\n",
      "\n",
      "def get_counter_from_names(names_to_count):\n",
      "    return {x: 0 for x in names_to_count}\n",
      "\n",
      "\n",
      "def generate_construct_counts(grna_matcher, construct_names, output_fp, fw_fastq_fp, rv_fastq_fp):\n",
      "    counts_info_tuple = _match_and_count_constructs_from_files(grna_matcher, construct_names, fw_fastq_fp, rv_fastq_fp)\n",
      "    counts_by_construct = counts_info_tuple[0]\n",
      "    counts_by_type = counts_info_tuple[1]\n",
      "    _write_counts(counts_by_construct, counts_by_type, output_fp)\n",
      "\n",
      "\n",
      "def _match_and_count_constructs_from_files(grna_matcher, construct_names, fw_fastq_fp, rv_fastq_fp):\n",
      "    construct_counts = get_counter_from_names(construct_names)\n",
      "    fw_fastq_handler = FastqHandler(fw_fastq_fp)\n",
      "    rv_fastq_handler = FastqHandler(rv_fastq_fp)\n",
      "    return _match_and_count_constructs(grna_matcher, construct_counts, fw_fastq_handler, rv_fastq_handler)\n",
      "\n",
      "\n",
      "def _match_and_count_constructs(grna_matcher, construct_counts, fw_fastq_handler, rv_fastq_handler):\n",
      "    summary_counts = {\"num_pairs\": 0, \"num_pairs_unrecognized\": 0, \"num_constructs_found\": 0,\n",
      "                      \"num_constructs_unrecognized\": 0, \"num_constructs_recognized\": 0}\n",
      "\n",
      "    paired_fastq_seqs = paired_fastq_generator(fw_fastq_handler, rv_fastq_handler)\n",
      "    for curr_pair_seqs in paired_fastq_seqs:\n",
      "        summary_counts[\"num_pairs\"] += 1\n",
      "        _report_progress(summary_counts[\"num_pairs\"])\n",
      "\n",
      "        grna_name_A, grna_name_B = grna_matcher.find_fw_and_rv_read_matches(*curr_pair_seqs)\n",
      "        if grna_name_A is not None and grna_name_B is not None:\n",
      "            # TODO: Note assumption that probe pair id is construct name!\n",
      "            # True in Mali screens, but if that won't be universal, must refactor here.\n",
      "            construct_name = compose_probe_pair_id_from_probe_ids(grna_name_A, grna_name_B)\n",
      "            summary_counts[\"num_constructs_found\"] += 1\n",
      "\n",
      "            if construct_name in construct_counts:\n",
      "                summary_counts[\"num_constructs_recognized\"] += 1\n",
      "                construct_counts[construct_name] += 1\n",
      "            else:\n",
      "                summary_counts[\"num_constructs_unrecognized\"] += 1\n",
      "                logging.debug(\"Unrecognized construct name: {0}\".format(construct_name))\n",
      "        else:\n",
      "            summary_counts[\"num_pairs_unrecognized\"] += 1\n",
      "            logging.debug(\"Unrecognized sequence: {0},{1}\".format(*curr_pair_seqs))\n",
      "\n",
      "    return construct_counts, summary_counts\n",
      "\n",
      "\n",
      "def _report_progress(num_fastq_pairs):\n",
      "    if num_fastq_pairs % 100000 == 0:\n",
      "        logging.info(\"On fastq pair number {0} at {1}\".format(num_fastq_pairs, datetime.datetime.now()))\n",
      "\n",
      "\n",
      "def _write_counts(counts_by_construct, counts_by_type, output_fp):\n",
      "    construct_names = list(counts_by_construct.keys())\n",
      "    construct_names.sort()\n",
      "\n",
      "    with open(output_fp, 'w') as file_handle:\n",
      "        summary_pieces = []\n",
      "        for curr_key, curr_value in counts_by_type.items():\n",
      "            summary_pieces.append(\"{0}:{1}\".format(curr_key, curr_value))\n",
      "        summary_comment = \",\".join(summary_pieces)\n",
      "        summary_comment = \"# \" + summary_comment\n",
      "        header = [get_construct_header(), \"counts\"]\n",
      "        writer = csv.writer(file_handle, delimiter=\"\\t\")\n",
      "\n",
      "        writer.writerow([summary_comment])\n",
      "        writer.writerow(header)\n",
      "        for curr_name in construct_names:\n",
      "            row = [curr_name, counts_by_construct[curr_name]]\n",
      "            writer.writerow(row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.malicrispr.construct_counter as ns_counter\n",
    "print(inspect.getsource(ns_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_constructs_for_one_fastq_pair(curr_base, run_prefix, seq_len, num_allowed_mismatches, constructs_fp, \n",
    "                                        output_dir, fw_fastq_fp, rv_fastq_fp):\n",
    "    construct_names, grna_name_seq_pairs = ns_extractor.extract_construct_and_grna_info(constructs_fp)\n",
    "    trimmed_grna_name_seq_pairs = ns_extractor.trim_probes(grna_name_seq_pairs, seq_len)\n",
    "    # Note: currently same value (num_allowed_mismatches) is being used for number of mismatches allowed in forward\n",
    "    # read and number of mismatches allowed in reverse read, but this can be altered if desired\n",
    "    grna_matcher = ns_matcher.GrnaPositionMatcher(trimmed_grna_name_seq_pairs, seq_len, num_allowed_mismatches, \n",
    "                                       num_allowed_mismatches)    \n",
    "    output_fp = ns_files.build_multipart_fp(output_dir, [curr_base, run_prefix, ns_counter.get_counts_file_suffix()])\n",
    "    ns_counter.generate_construct_counts(grna_matcher, construct_names, output_fp, fw_fastq_fp, rv_fastq_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel processing at 2017-03-23 14:53:15.988404\n",
      "Starting A549-CV4-100-d21-1_S3_L001_001_trimmed53_len_filtered at 2017-03-23 14:53:16.005416\n",
      "Starting A549-CV4-100-d21-2_S4_L001_001_trimmed53_len_filtered at 2017-03-23 14:53:16.005721\n",
      "Starting A549-CV4-100-d28-1_S5_L001_001_trimmed53_len_filtered at 2017-03-23 14:53:16.006993\n",
      "A549-CV4-100-d21-2_S4_L001_001_trimmed53_len_filtered elapsed time: 0:00:00\n",
      "Starting A549-CV4-100-d28-2_S6_L001_001_trimmed53_len_filtered at 2017-03-23 14:53:16.742590\n",
      "A549-CV4-100-d28-1_S5_L001_001_trimmed53_len_filtered elapsed time: 0:00:00\n",
      "A549-CV4-100-d21-1_S3_L001_001_trimmed53_len_filtered elapsed time: 0:00:00\n",
      "A549-CV4-100-d28-2_S6_L001_001_trimmed53_len_filtered elapsed time: 0:00:00\n",
      "parallel processing elapsed time: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "import ccbbucsd.utilities.parallel_process_fastqs as ns_parallel\n",
    "\n",
    "g_parallel_results = ns_parallel.parallel_process_paired_reads(g_filtered_fastqs_dir, \n",
    "    ns_filter.get_filtered_file_suffix(), g_num_processors, count_constructs_for_one_fastq_pair, \n",
    "    [g_fastq_counts_run_prefix, g_len_of_seq_to_match, g_num_allowed_mismatches, g_library_fp,\n",
    "     g_fastq_counts_dir], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A549-CV4-100-d21-1_S3_L001_001_trimmed53_len_filtered: finished\n",
      "A549-CV4-100-d21-2_S4_L001_001_trimmed53_len_filtered: finished\n",
      "A549-CV4-100-d28-1_S5_L001_001_trimmed53_len_filtered: finished\n",
      "A549-CV4-100-d28-2_S6_L001_001_trimmed53_len_filtered: finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ns_parallel.concatenate_parallel_results(g_parallel_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
